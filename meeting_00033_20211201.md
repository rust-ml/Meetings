# Rust ML IG Meeting 00033

## Meeting Info

Date: 20211201

Start time: [1100ET](http://timesched.pocoo.org/?date=2021-12-01&tz=utc!,gb:london,de:berlin,us:new-york-city:ny,us:austin:tx,us:seattle:wa,cn:beijing&range=960,1080)

Zoom: https://fau.zoom.us/j/67809173119 

## Agenda

- https://github.com/Rust-GPU/Rust-CUDA
    - Implicit memory hierarchy
- Enzyme working on moving into rustc
- linfa
    - mixed typed datasets
    - Naive Bayes with custom distribution parametrizations (Multinomial/Bernoulli Naive Bayes)
    

## Participants

 - Manuel
 - Lorenz
 - Riccardo
 - Qing
 - Yuhan

## Minutes

 - Rust-CUDA
     - codegen for compiling Rust to PTX and runner
     - wrapping existing CUDA librarys for Rust
     - cuDNN offers architecture independent performance
     - PTX uses unbounded number of virtual register -> can be optimized on-the-fly for specific architecture (AMD assembly is more architecture specific)
 - custom codegen driver
     - communication overhead when establishing dynamic library support to cg_llvm

 - automatic GPU offloading
     - very hard, even harder for asynchronous accelerator architecutres
     - working for specific problems, but not applicable to real-world problems
     - Polly kind supports that, but interesting for better optimization passes
 - Further questions about Rust-CUDA
     - memory architecture -> globaly, shared, local memory how does the compiler differentiate
     - launching kernels requires mutable pointers (because of ownership management of Rust) and is therefore unsafe
 - Linfa
     - once GAT are stabilized, integrate `space` crate into Linfa
     - approximate DBSCAN still slower than standard DBSCAN
     - extended Naive Bayes depends on mixed typed datasets and generic distribution parametrization

## Actions

 - look at rustc llvm codegen
 - create dummy integration of Rust-CUDA into `ndarray`
 - mixed type datasets RFC for Linfa
